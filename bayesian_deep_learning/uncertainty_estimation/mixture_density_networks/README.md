## Mixture Density Networks
En est√° p√°gina se explican brevemente **las redes de densidad mixta** como soluci√≥n t√©cnica para aproximar distribuciones heterog√©neas de la variable respuesta. Este es el caso en el que sabemos que los datos u observaciones provienen de fuentes o procesos diferentes conocidos. Para ello, primero se incluye una breve introducci√≥n a **modelos de mixturas** que es a la familia a la que pertence este tipo de redes.


### Indice de contenidos
- [Introducci√≥n a la t√©cnica](#introduccion)
   - [Motivaci√≥n](#motivacion) 
   - [Implementaci√≥n](#implementacion) 
- [Mixture Density Networks](#mdn)

- [Experimentos y conclusiones](#Experimentos-y-conclusiones) 

<a name="introduccion"></a>
## Introducci√≥n

Un **modelo de mixturas** es un modelo probabil√≠stico que nos permite representar la presencia de subpoblaciones de la poblaci√≥n general. Esta representaci√≥n de subpoblaciones nos va a permitir construir un estimador m√°s robusto en el caso en el que la distribuci√≥n de la variable respuesta sea heterog√©nea

En el siguiente gr√°fico tenemos un ejemplo de variable heterog√©nea *y* (el precio medio de unos cascos). Viendo este gr√°fico podemos inferir que podr√≠amos tener 3 tipos de cascos diferentes: uno de precio bajo ~$30, medio ~$60, y alto ~$120. Sin embargo, las variables o procesos que causan realmente estas fluctaciones de precios no las tenemos ni las conocemos (son variables latentes).   

<p align="center"><img src="./img/mixture_models.png" height="160" alt="Mixture Density Network" /></p>
<p align="center">Mixture Density Network</p>

<a name="motivacion"></a>
### Motivaci√≥n 

Si modelaramos este tipo de incertidumbre mediante t√©cnicas como ['on-the-fly'](https://github.com/beeva/TEC_LAB-bayesian_probabilistic/tree/master/bayesian_deep_learning/uncertainty_estimation#1---al-vuelo) obtendriamos la distribucci√≥n de la izquierda, donde ignoramos las distintas fuentes que causan las fluctaciones en el precio, aunque la incertidumbre global seguir√≠a siendo v√°lida. Por el contrario, los modelos de mixturas si son capaces de modelar distitnas fuentes de incertidumbre y aproximar una distribuci√≥n N modal (que contiene N modos) que se ajuste m√°s a la distribucci√≥n real.

<a name="implementacion"></a>
### Implementaci√≥n 

El problema que surje al estimar la distribucci√≥n de *y* es que por un lado no conocemos los distintos procesos que generan esas subpoblaciones ni a que subpoblaci√≥n pertenece una observaci√≥n concreta. Para esto, se utiliza el algoritmo de Maximum Likelihood Estimation que se explica en el siguiente [post de referencia](https://towardsdatascience.com/gaussian-mixture-models-and-expectation-maximization-a-full-explanation-50fa94111ddd).

<a name="MLE"></a>
#### MLE - Maximum Likelihood Estimation

El algoritmo de MLE o m√°xima verosimilitud nos permite obtener los par√°metros del modelo o distribuci√≥n que maximizan la probabibilidad de obtener unos datos dados.

Referencia - [Ejemplo de c√°lculo de MLE para la implementaci√≥n de la funci√≥n de p√©rdida](https://towardsdatascience.com/maximum-likelihood-estimation-explained-normal-distribution-6207b322e47f#:~:text=%E2%80%9CA%20method%20of%20estimating%20the,observed%20data%20is%20most%20probable.%E2%80%9D&text=Let's%20say%20we%20have%20some,that%20it%20is%20normally%20distributed)


<a name="mdn"></a>
## Mixture Density Networks

Las **redes de densidad mixta** (Bishop, 1994) es un tipo de red que combina las redes convencionales con el concepto de *modelo de mixturas*. En este modelo la s√°lida de la DNN hace la estimaci√≥n de par√°metros para la familia de distribuciones o componentes seleccionadas, las cuales se suman teniendo en cuenta el coeficiente de mezcla ‚ç∫ para obtener finalmente una distribucci√≥n condicional het√©rogena de *y* respecto a la entrada: 

<p align="center"><img src="./img/MDN.png" height="160" alt="Mixture Density Network" /></p>
<p align="center">Mixture Density Network</p>

Formalmente, la probabilidad condicionada de una red de mixturas tiene la siguiente forma:

<p align="center"><img src="./img/mdn_formula.png" height="70" alt="Formula MDN" /></p>
<p align="center">Mixture Density Network</p>

En esta f√≥rmula los par√°metros tienen la siguiente sem√°ntica:

* **c se corresponde con el √≠ndice de la correspondiente mixtura**. Hay hasta C componentes de mixtura (e.g. distribuciones) por salida, siendo un par√°metro seleccionable.
* **‚ç∫ es el coeficiente de mezcla**. Para entender este coeficiente podemos imaginarnos los controles deslizantes que controlan la mezcla de C salidas diferentes de audio. Este par√°metro esta condicionado por la entrada x.
* **ùíü  esta es la correspondiente distribuci√≥n de entrada a ser mezclada**. La distribuci√≥n puede ser elegida atendiendo al tipo de aplicaci√≥n.
* **Œª son los par√°metros de la distribuci√≥n ùíü**. En el caso de que denotemos ùíü como una distribuci√≥n gausiana, estos parametros se corresponderian Œª1 a la media condicional mean Œº(x) y 
Œª2 a la desviaci√≥n est√°ndar œÉ(x). Las distribuciones pueden tener distinto n√∫mero de par√°metros (e.g.: Bernoulli and Chi2 tienen 1 par√°metro, Beta tiene 2, y la gaussiana truncada tiene hasta 4 par√°metros) Estos son par√°metros que forman tambi√©n la salida de la red.


### Aplicaciones

Entre las **aplicaciones m√°s destacadas** se encuentra la de Apple‚Äôs Siri en iOS 11 para reconocimiento de voz[2]. En [3] se puede ver su aplicaci√≥n en generaci√≥n de manuscritos y Amazon Forecast lo tiene dentro de la suite de algoritmos incluidos en su plataforma.

### Ventajas y desventajas

* *Ventajas* 

Permite estimar distribuciones heterog√©neas de la variable respuesta.

* *Desventajas* 

El tama√±o de la s√°lida de la red creada por la capa final de la MDN es (c+2)* m, d√≥nde c es la dimensi√≥n de s√°lida de la red convencional y m el n√∫mero de mixturas que estamos usando. Esto supone un incremento considerable en la dimensi√≥n de s√°lida respecto a la red convencional lo que puede volverlas muy inestables.

### Experimentos y conclusiones
   
Este m√©todo, en contraposici√≥n con lo validado en el Exp.I de [estimaci√≥n de incertidumbre al vuelo](https://github.com/beeva/TEC_LAB-bayesian_probabilistic/tree/master/bayesian_deep_learning/uncertainty_estimation#1---al-vuelo), presentan las siguientes ventajas que se pueden resumir a mayor libertad en la definici√≥n del prior (condici√≥n de entorno o asunci√≥n dada):

 - Permite modelar facilmente que el ruido provenga de distintas familias de distribucciones.
 - Pueden modelar ruido multimodal, es decir, que no s√≥lo provenga de una sola distribuci√≥n si no de la suma de varias distribucciones de la misma familia con distintos par√°metros. Este prior, sin embargo, tambi√©n esta implicito en el exp.I y no es f√°cilmente modificable.
 - Tienen m√°s soporte, es decir, el m√©todo est√° m√°s comunmente aceptado. 

Se puede encontrar un [notebook](experiments/V0.1.6-real_datasets/uncertainty_prediction_house_prices_mdn.ipynb) con esta t√©cnica aplicadas a un dataset para la estimaci√≥n de incertidumbre.

En detalle, se puede encontrar un [notebook de redes de densidad mixta (MDN)](experiments/V3.0.0-mixture_density_networks), donde se puede encontrar una implementaci√≥n y el detalle de las conclusiones.


#### Referencias

[1] https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca

[2] Siri Team, Deep Learning for Siri‚Äôs Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis (2017)

[3] Alex Graves, Generating Sequences With Recurrent Neural Networks (2014)
