{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MI-BAY] Estimating error prediction\n",
    "\n",
    "## List of experiments\n",
    "* [01 Original experiment](#01-Original-experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 42  # Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "experiments = {}  # Where experiments will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "Maybe this should go in a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_gen_data(n_train, f_xy, f_noise, n_test=0, *args, **kwargs):\n",
    "    \"\"\"Generate samples and labels for train and test.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    n_train: int\n",
    "        Number of train samples to generate.\n",
    "    f_xy: function()-> ( [feature1, feature2, ...], [target1, target2, etc] )\n",
    "        Function that generates a tuple of 2 lists, features and targets.\n",
    "        NOTE: features and targets can be tensors.\n",
    "    f_noise: function([features]) -> [features]\n",
    "        Function that adds noise to the features\n",
    "    n_test: int\n",
    "        Number of test samples to generate.\n",
    "        \n",
    "    RETURN\n",
    "    ------\n",
    "    ( ([train features], [train targets]), ([test features], [test targets]) )\n",
    "    \"\"\"\n",
    "    assert(n_test>=0)\n",
    "    n_samples = n_train + n_test\n",
    "    x,y = f_xy_lineal(n_samples)\n",
    "    # Add noise\n",
    "    noise = f_noise(x)\n",
    "    y_noise = tuple([y+noise for y, noise in zip(y, noise) ])\n",
    "    i = np.random.shuffle(np.arange(n_samples))\n",
    "    return(\n",
    "        (  # Train data\n",
    "            [x_feat[:n_train] for x_feat in x],\n",
    "            [y_labl[:n_train] for y_labl in y_noise]\n",
    "        ),\n",
    "        (  # Test data\n",
    "            [x_feat[n_train:] for x_feat in x],\n",
    "            [y_labl[n_train:] for y_labl in y_noise]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils.generic_utils import to_list\n",
    "\n",
    "def train_step(model, optimizer, f_loss, x_train, y_train):\n",
    "    \"\"\"Training step for a model.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = to_list(model(x_train, training=True))\n",
    "        loss = f_loss(y_train, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Original experiment\n",
    "\n",
    "In this experiment we will try to measure the uncertainty the predictions of a machine learning model.\n",
    "It is framed into our interest for bayesian methods and in particular bayesian deep learning.\n",
    "\n",
    "\n",
    "The experiment is based on the work of Steve Thorn.\n",
    "#### References\n",
    "* Post: [Predicting uncertainty with neural networks](https://medium.com/@steve_thorn/predicting-uncertainty-with-neural-networks-aec0217eb37d)\n",
    "* [Original notebook](https://github.com/sthorn/deep-learning-explorations/blob/master/predicting-uncertainty-variance.ipynb)\n",
    "* [Local copy of the original notebook](01-Original.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I will save the data for the original experiment\n",
    "experiments['original'] = {\n",
    "    'name': '01-original'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the synthetic data\n",
    "\n",
    "Using a synthetic dataset is useful since we can fully control the data generation phenomenon. This way we can test the potentiality of the method.\n",
    "\n",
    "Dataset properties:\n",
    "\n",
    "* $x \\in \\mathbb{R} \\in  [0,1)$ sampled uniformly.\n",
    "* $y \\in \\mathbb{R} $\n",
    "\n",
    "The relationship is $y = 2x+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relationship01(x):\n",
    "    return 2*x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_xy_lineal(n_samples):\n",
    "    \"\"\"Generate samples-labels.\n",
    "    \n",
    "    RETURN:\n",
    "     Tuple of tuples (inputs, outputs), \n",
    "    \"\"\"\n",
    "    x = tf.random.uniform(\n",
    "        shape=(n_samples,1),\n",
    "        minval=0.0,\n",
    "        maxval=1.0\n",
    "    )\n",
    "    return(([x],[relationship01(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding uncertainty\n",
    "\n",
    "Since the previous relationship is easily captured by a linear model (i.e. a linear regression), we will add some noise to add uncertainty in the predictions.\n",
    "\n",
    "The noise is sampled from a [gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution) with a mean of $0$ and a variance of $1$.\n",
    "\n",
    "That is $\\mathcal{N}(\\mu=0.0,\\sigma^2=1.0)$\n",
    "\n",
    "\n",
    "##### Heteroscedasticity\n",
    "The problem with this noise is that is equally distributed along the dataset. So the prediction error will be homogeneus, it fullfils the assumption of [homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity) and the uncertainty could also be measured with a simple linear regression.\n",
    "\n",
    "So in order to make the error ($\\varepsilon$) difficult to measure we will make it dependant on the $x$ value in a periodic manner.\n",
    "\n",
    "$\\varepsilon = \\sin(4*\\pi*x) * 0.4 * \\mathcal{N}(0.0,1.0)$\n",
    "\n",
    "Factors explanation\n",
    "1. Relative scale based on the $x$ value (this adds the heteroscedasticity). It is also periodic.\n",
    "1. Absolute scale.\n",
    "1. Gaussian noise.\n",
    "\n",
    "\n",
    "##### Considerations\n",
    "1. Every time we sample from the same $x$, there will be different a different $y$.\n",
    "1. If we make a histogram with $y$ values sampled from the same $x$ it will be a normal distribution centered on the true value of $x$.\n",
    "1. The error is periodic, not completely [stochastic](https://en.wikipedia.org/wiki/Stochastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_noise_gaussian_periodic(samples_x):\n",
    "    \"\"\"Generate noise for labels\n",
    "    \n",
    "    RETURN: \n",
    "     Tuple of tuples (noise for outputs)\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal(\n",
    "        shape=samples_x[0].shape,\n",
    "        mean=0.0,\n",
    "        stddev=1.0\n",
    "    )* 0.4 * np.sin(samples_x[0]*4*np.pi)\n",
    "    return([noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments['original']['data'] = {\n",
    "    'n_train': 10000,\n",
    "    'n_test': 2000,\n",
    "    'f_xy': f_xy_lineal,\n",
    "    'f_noise': f_noise_gaussian_periodic\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = f_gen_data(**experiments['original']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the data\n",
    "\n",
    "The blue dots are training data, and the red dots are testing data, we see they come from the same population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(train_x[0], train_y[0], '.', color='blue', alpha=0.1)\n",
    "plt.plot(test_x[0], test_y[0], '.', color='red', alpha=0.2)\n",
    "plt.xlabel(r'$x$');\n",
    "plt.ylabel(r'$y$');\n",
    "plt.legend([r'Train data','Test data'], loc='upper left', )\n",
    "plt.title('Synthetic data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The candidate model is an [ANN](https://en.wikipedia.org/wiki/Artificial_neural_network), since it can model non-linearities it should be able to fit an error estimation.\n",
    "\n",
    "There will be 2 outputs.\n",
    "* y_pred: the prediction $\\hat{y}$\n",
    "* y_var: the prediction error variance $(y-\\hat{y})^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model01():\n",
    "    in_x = tf.keras.Input(\n",
    "        shape=(1,),\n",
    "        name=\"x\"\n",
    "    )\n",
    "    f1 = tf.keras.layers.Dense(\n",
    "        1000,\n",
    "        name=\"hidden\",\n",
    "        activation=tf.keras.activations.relu\n",
    "    )(in_x)\n",
    "    out_y = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        name=\"y_pred\",\n",
    "        activation=tf.keras.activations.linear\n",
    "    )(f1)\n",
    "    out_var = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        name=\"y_var\",\n",
    "        activation=tf.keras.activations.linear\n",
    "    )(f1)\n",
    "    return tf.keras.Model(inputs=[in_x], outputs=[out_y, out_var], name=\"model01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments['original']['model'] =  get_model01()\n",
    "experiments['original']['model'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "We have 4002 parameters to fit 10000 data points and the uncertainty in the estimation. Since the relationship is linear $y = ax + b$ (only 2 parameters needed to predict $x$, or y_pred), there are 4000 parameters dedicated to the error prediction, that is a rate of 2 free parameters to 5 data points. Overfitting warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "We will use [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss01(y_true, y_pred):\n",
    "    y_pred_sqr_error = (y_true[0]-y_pred[0])**2\n",
    "    return tf.reduce_mean(\n",
    "        y_pred_sqr_error +\n",
    "        (y_pred_sqr_error - y_pred[1])**2 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments['original']['loss'] = loss01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small test\n",
    "pred_y = experiments['original']['model'].predict(train_x)\n",
    "assert(pred_y[0].shape==pred_y[1].shape)\n",
    "print(pred_y[0].shape)\n",
    "print(loss01(train_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments['original']['hyper'] = {\n",
    "    'optimizer': tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-4,\n",
    "    ),\n",
    "    'bs': experiments['original']['data']['n_train'],  # Whole dataset\n",
    "    'epochs': 10000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classic Keras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "experiments['original']['model'].compile(\n",
    "    optimizer=experiments['original']['hyper']['optimizer'],\n",
    "    loss=experiments['original']['loss'],\n",
    "    metrics=[tf.keras.metrics.MSE]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "If I try...\n",
    "```python\n",
    "experiments['original']['model'].fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=experiments['original']['hyper']['bs'],\n",
    "    epochs=experiments['original']['hyper']['epochs'],\n",
    ")\n",
    "```\n",
    "\n",
    "Keras complains the target shape is not as expected.\n",
    "```\n",
    "ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<tf.Tensor:...\n",
    "```\n",
    "\n",
    "`F**k` keras, do the check on the returned value from the loss function!!!! Not on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Au naturel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=experiments['original']['model']\n",
    "f_loss=experiments['original']['loss']\n",
    "optimizer = experiments['original']['hyper']['optimizer']\n",
    "batch_size  = experiments['original']['hyper']['bs']\n",
    "epochs = experiments['original']['hyper']['epochs']\n",
    "x_train = train_x\n",
    "y_train = train_y\n",
    "\n",
    "# Function\n",
    "\n",
    "len_dataset = len(x_train[0])\n",
    "indices = tf.range(len_dataset) # All indices for the dataset samples\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tf.random.shuffle(indices)\n",
    "    i_batch = 0\n",
    "    while i_batch<len_dataset:\n",
    "        batch_indices = indices[i_batch:i_batch+batch_size]  # Stochastic batch selection\n",
    "        loss = train_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            f_loss,\n",
    "            x_train=[\n",
    "                tf.gather(params=feat_x, indices=batch_indices)\n",
    "                for feat_x in x_train\n",
    "            ],\n",
    "            y_train=[\n",
    "                tf.gather(params=feat_y, indices=batch_indices)\n",
    "                for feat_y in y_train\n",
    "            ],\n",
    "        )\n",
    "        i_batch += batch_size\n",
    "    if epoch%100==0:\n",
    "        print(\"Epoch: {}\\tLoss: {}\".format(epoch,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments['original']['loss'](\n",
    "    train_y,\n",
    "    experiments['original']['model'].predict(train_x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments['original']['loss'](\n",
    "    test_x,\n",
    "    experiments['original']['model'].predict(test_x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = experiments['original']['model'].predict(train_x)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(train_x[0], train_y[0], '.', color='blue', alpha=0.1)\n",
    "#plt.plot(train_x[0], pred_y[0], '.', color='green', alpha=0.1)\n",
    "plt.errorbar(train_x[0], pred_y[0], yerr=pred_y[1], fmt='.', color='green', alpha=0.1);\n",
    "plt.xlabel(r'$x$');\n",
    "plt.ylabel(r'$y$');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = experiments['original']['model'].predict(test_x)\n",
    "print(loss01(test_y, pred_y))\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(test_x[0], test_y[0], '.', color='red', alpha=0.1)\n",
    "plt.errorbar(test_x[0], pred_y[0], yerr=tf.sqrt(pred_y[1]), fmt='.', color='green', alpha=0.1);\n",
    "plt.xlabel(r'$x$');\n",
    "plt.ylabel(r'$y$');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model to fit "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO:\n",
    "Plot\n",
    "Gaussian tests against random points\n",
    "\n",
    "Relación con Bayesiano\n",
    " - ¿Dónde están los priors?\n",
    " - Parámetros de función\n",
    " - Prueba de aceptación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = f_xy_lineal(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
