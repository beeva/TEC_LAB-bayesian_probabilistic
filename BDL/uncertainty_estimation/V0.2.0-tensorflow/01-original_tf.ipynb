{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating error prediction\n",
    "\n",
    "## List of experiments\n",
    "* [Original experiment](#Original-experiment)\n",
    "* [Minibatches](#Minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 42  # Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "experiments = {}  # Where experiments will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "Maybe this should go in a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_gen_data(n_train, f_xy, f_noise, n_test=0, *args, **kwargs):\n",
    "    \"\"\"Generate samples and labels for train and test.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    n_train: int\n",
    "        Number of train samples to generate.\n",
    "    f_xy: function()-> ( [feature1, feature2, ...], [target1, target2, etc] )\n",
    "        Function that generates a tuple of 2 lists, features and targets.\n",
    "        NOTE: features and targets can be tensors.\n",
    "    f_noise: function([features]) -> [features]\n",
    "        Function that adds noise to the features\n",
    "    n_test: int\n",
    "        Number of test samples to generate.\n",
    "        \n",
    "    RETURN\n",
    "    ------\n",
    "    ( ([train features], [train targets]), ([test features], [test targets]) )\n",
    "    \"\"\"\n",
    "    assert(n_test>=0)\n",
    "    n_samples = n_train + n_test\n",
    "    x,y = f_xy_lineal(n_samples)\n",
    "    # Add noise\n",
    "    noise = f_noise(x)\n",
    "    y_noise = tuple([y+noise for y, noise in zip(y, noise) ])\n",
    "    i = np.random.shuffle(np.arange(n_samples))\n",
    "    return(\n",
    "        (  # Train data\n",
    "            [x_feat[:n_train] for x_feat in x],\n",
    "            [y_labl[:n_train] for y_labl in y_noise]\n",
    "        ),\n",
    "        (  # Test data\n",
    "            [x_feat[n_train:] for x_feat in x],\n",
    "            [y_labl[n_train:] for y_labl in y_noise]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils.generic_utils import to_list\n",
    "\n",
    "def train_step(model, optimizer, f_loss, x_train, y_train):\n",
    "    \"\"\"Training step for a model.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = to_list(model(x_train, training=True))\n",
    "        loss = f_loss(y_train, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dists(*dists):\n",
    "    \"\"\"Scatter-Plot 2 distributions\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    dists: [([float],[float],string) ]\n",
    "        x points, y points and color    \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14,6))\n",
    "    for (x,y,color) in dists:\n",
    "        plt.plot(x, y, '.', color=color, alpha=0.1)\n",
    "    plt.xlabel(r'$x$');\n",
    "    plt.ylabel(r'$y$');\n",
    "    plt.legend([r'Train data','Test data'], loc='upper left', )\n",
    "    plt.title('Synthetic data')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(x, y_pred, y_var, y_true):\n",
    "    y_var_margin = 3*(y_var**0.5)  # 3 std deviations captures 99.7% of samples, conservative margin\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.plot(x, y_true, '.', color='blue', alpha=0.1)\n",
    "    plt.errorbar(x, y_pred, yerr=y_var_margin, fmt='.', color='green', alpha=0.1);\n",
    "    plt.xlabel(r'$x$');\n",
    "    plt.ylabel(r'$y$');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, loss, true_x, true_y):\n",
    "    pred = model.predict(true_x)\n",
    "    print(\"Loss: {}\".format(loss(true_y, pred)))\n",
    "    plot_predictions(x=true_x[0], y_pred=pred[0], y_var=pred[1], y_true=true_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_callback_basic(epoch, loss, *args):\n",
    "    if epoch%100==0:\n",
    "        print(\"Epoch: {}\\tLoss: {}\".format(epoch,loss))\n",
    "\n",
    "        \n",
    "\n",
    "def train(model, f_loss, optimizer, batch_size, epochs, x_train, y_train, f_callback=None):\n",
    "    \"\"\"Train a model with stochastic gradient descent.\n",
    "    \"\"\"\n",
    "\n",
    "    len_dataset = len(x_train[0])\n",
    "    indices = tf.range(len_dataset) # All indices for the dataset samples\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tf.random.shuffle(indices)\n",
    "        i_batch = 0\n",
    "        while i_batch<len_dataset:\n",
    "            batch_indices = indices[i_batch:i_batch+batch_size]  # Stochastic batch selection\n",
    "            loss = train_step(\n",
    "                model,\n",
    "                optimizer,\n",
    "                f_loss,\n",
    "                x_train=[\n",
    "                    tf.gather(params=feat_x, indices=batch_indices)\n",
    "                    for feat_x in x_train\n",
    "                ],\n",
    "                y_train=[\n",
    "                    tf.gather(params=feat_y, indices=batch_indices)\n",
    "                    for feat_y in y_train\n",
    "                ],\n",
    "            )\n",
    "            i_batch += batch_size\n",
    "        if f_callback:\n",
    "            f_callback(epoch, loss, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = dict()  # To save all the data from the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original experiment\n",
    "\n",
    "In this experiment we will try to measure the uncertainty the predictions of a machine learning model.\n",
    "It is framed into our interest for bayesian methods and in particular bayesian deep learning.\n",
    "\n",
    "\n",
    "The experiment is based on the work of Steve Thorn. The idea is to add an output, to a neural network, that gives an estimation of the uncertainty of the prediction.\n",
    "\n",
    "\n",
    "#### References\n",
    "* Post: [Predicting uncertainty with neural networks](https://medium.com/@steve_thorn/predicting-uncertainty-with-neural-networks-aec0217eb37d)\n",
    "* [Original notebook](https://github.com/sthorn/deep-learning-explorations/blob/master/predicting-uncertainty-variance.ipynb)\n",
    "* [Local copy of the original notebook](01-Original.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I will save the data for the original experiment\n",
    "experiments['original'] = {\n",
    "    'name': '01-original'\n",
    "}\n",
    "\n",
    "experiment = experiments['original']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the synthetic data\n",
    "\n",
    "Using a synthetic dataset is useful since we can fully control the data generation phenomenon. This way we can test the potentiality of the method.\n",
    "\n",
    "Dataset properties:\n",
    "\n",
    "* $x \\in \\mathbb{R} \\in  [0,1)$ sampled uniformly.\n",
    "* $y \\in \\mathbb{R} $\n",
    "\n",
    "The relationship is $y = 2x+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relationship01(x):\n",
    "    return 2*x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_xy_lineal(n_samples):\n",
    "    \"\"\"Generate samples-labels.\n",
    "    \n",
    "    RETURN:\n",
    "     Tuple of tuples (inputs, outputs), \n",
    "    \"\"\"\n",
    "    x = tf.random.uniform(\n",
    "        shape=(n_samples,1),\n",
    "        minval=0.0,\n",
    "        maxval=1.0\n",
    "    )\n",
    "    return(([x],[relationship01(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our synthetic data without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = f_xy_lineal(1000)\n",
    "plot_dists((x[0],y[0],'blue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding uncertainty\n",
    "\n",
    "Since the previous relationship is easily captured by a linear model (i.e. a linear regression), we will add some noise to add uncertainty in the predictions.\n",
    "\n",
    "The noise is sampled from a [gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution) with a mean of $0$ and a variance of $1$.\n",
    "\n",
    "That is $\\mathcal{N}(\\mu=0.0,\\sigma^2=1.0)$\n",
    "\n",
    "\n",
    "##### Heteroscedasticity\n",
    "The problem with this noise is that is equally distributed along the dataset. So the prediction error will be homogeneus, it fullfils the assumption of [homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity) and the uncertainty could also be measured with a simple linear regression.\n",
    "\n",
    "So in order to make the error ($\\varepsilon$) difficult to measure we will make it dependant on the $x$ value in a periodic manner.\n",
    "\n",
    "$\\varepsilon = \\mathcal{N}(\\mu=0.0, \\sigma^2= (\\left| \\sin(4*\\pi*x) \\right| * 0.4)^2)$\n",
    "\n",
    "Factors explanation\n",
    "1. Relative scale based on the $x$ value (this adds the heteroscedasticity). It is also periodic.\n",
    "1. Absolute scale.\n",
    "1. Gaussian noise.\n",
    "\n",
    "\n",
    "##### Considerations\n",
    "1. Every time we sample from the same $x$, there will be different a different $y$.\n",
    "1. If we make a histogram with $y$ values sampled from the same $x$ it will be a normal distribution centered on the true value of $x$.\n",
    "1. The error is periodic, not completely [stochastic](https://en.wikipedia.org/wiki/Stochastic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the variance changes depending on the feature (x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the period on the variance of the noise\n",
    "\n",
    "x_sin  = np.linspace(0,1, 1000)\n",
    "y_sin = (np.abs(np.sin(x_sin * 4 * np.pi)) * 0.4)**2\n",
    "\n",
    "plot_dists( (x_sin, y_sin, 'green') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_noise_gaussian_periodic(samples_x):\n",
    "    \"\"\"Generate noise for labels\n",
    "    \n",
    "    RETURN: \n",
    "     Tuple of tuples (noise for outputs)\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal(\n",
    "        shape=samples_x[0].shape,\n",
    "        mean=0.0,\n",
    "        stddev=np.abs(np.sin(samples_x[0] * 4 * np.pi)) * 0.4\n",
    "    )\n",
    "    return([noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment['data'] = {\n",
    "    'n_train': 10000,\n",
    "    'n_test': 2000,\n",
    "    'f_xy': f_xy_lineal,\n",
    "    'f_noise': f_noise_gaussian_periodic\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = f_gen_data(**experiment['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the data\n",
    "\n",
    "The blue dots are training data, and the red dots are testing data, we see they come from the same population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dists(\n",
    "    (train_x[0], train_y[0], 'blue'),\n",
    "    (test_x[0], test_y[0], 'red')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The candidate model is an [ANN](https://en.wikipedia.org/wiki/Artificial_neural_network), since it can model non-linearities it should be able to fit an error estimation.\n",
    "\n",
    "There will be 2 outputs.\n",
    "* y_pred: the prediction $\\hat{y}$\n",
    "* y_var: the prediction error variance $(y-\\hat{y})^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model01():\n",
    "    in_x = tf.keras.Input(\n",
    "        shape=(1,),\n",
    "        name=\"x\"\n",
    "    )\n",
    "    f1 = tf.keras.layers.Dense(\n",
    "        1000,\n",
    "        name=\"hidden\",\n",
    "        activation=tf.keras.activations.relu\n",
    "    )(in_x)\n",
    "    out_y = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        name=\"y_pred\",\n",
    "        activation=tf.keras.activations.linear\n",
    "    )(f1)\n",
    "    out_var = tf.keras.layers.Dense(\n",
    "        1,\n",
    "        name=\"y_var\",\n",
    "        activation=tf.keras.activations.linear\n",
    "    )(f1)\n",
    "    return tf.keras.Model(inputs=[in_x], outputs=[out_y, out_var], name=\"model01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment['model'] =  get_model01()\n",
    "experiment['model'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "We have 4002 parameters to fit 10000 data points and the uncertainty in the estimation. Since the relationship is linear $y = ax + b$ (only 2 parameters needed to predict $x$, or y_pred), there are 4000 parameters dedicated to the error prediction, that is a rate of 2 free parameters to 5 data points. Overfitting warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "We will use [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) as the loss function. As there are two outputs the total loss will be the mean of the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss01(y_true, y_pred):  # Candidate 1\n",
    "    y_pred_sqr_error = (y_true[0]-y_pred[0])**2\n",
    "    return tf.reduce_mean(\n",
    "        y_pred_sqr_error +\n",
    "        (y_pred_sqr_error - y_pred[1])**2 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss02(y_true, y_pred):  # Candidate 2\n",
    "    y_pred_sqr_error = (y_true[0]-y_pred[0])**2\n",
    "    return (\n",
    "        tf.reduce_mean(y_pred_sqr_error) +\n",
    "        tf.reduce_mean((y_pred_sqr_error - y_pred[1])**2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pick the fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = experiment['model'].predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "loss01(train_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "loss02(train_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment['loss'] = loss01  # Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss01(train_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss02(train_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small test\n",
    "assert(loss01(train_y, pred_y)-loss02(train_y, pred_y)<0.00001)  # Same value\n",
    "assert(pred_y[0].shape==pred_y[1].shape)\n",
    "print(pred_y[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment['hyper'] = {  # Hyperparameters\n",
    "    'optimizer': tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-4,\n",
    "    ),\n",
    "    'bs': experiments['original']['data']['n_train'],  # Whole dataset\n",
    "    'epochs': 6000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classic Keras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "experiments['original']['model'].compile(\n",
    "    optimizer=experiments['original']['hyper']['optimizer'],\n",
    "    loss=experiments['original']['loss'],\n",
    "    metrics=[tf.keras.metrics.MSE]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "If I try...\n",
    "```python\n",
    "experiments['original']['model'].fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=experiments['original']['hyper']['bs'],\n",
    "    epochs=experiments['original']['hyper']['epochs'],\n",
    ")\n",
    "```\n",
    "\n",
    "Keras complains the target shape is not as expected.\n",
    "```\n",
    "ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [<tf.Tensor:...\n",
    "```\n",
    "\n",
    "`F**k` keras, do the check on the returned value from the loss function!!!! Not on the training data.\n",
    "\n",
    "*NOTE: the problem is train_y.shape is not the same as the shape from the otput model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Au naturel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    model=experiment['model'],\n",
    "    f_loss=experiment['loss'],\n",
    "    optimizer=experiment['hyper']['optimizer'],\n",
    "    batch_size=experiment['hyper']['bs'],\n",
    "    epochs=experiment['hyper']['epochs'],\n",
    "    x_train=train_x,\n",
    "    y_train=train_y,\n",
    "    f_callback=f_callback_basic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(experiment['model'], experiment['loss'], train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(experiment['model'], experiment['loss'], test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "I have found **NO EVIDENCE** to say the technique works. The captured variance is homoscedastic, the confidence margins per point are the same all along the dataset.\n",
    "\n",
    "This causes a disonance because the [original implementation](00-original_pytorch.ipynb) in Pytorch seems to work differently. But, as I have not been able to replicate the behaviour I can't continue that path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next figure shows the **captured variance** per point, the shape should resemble the curve we build in the section [Adding uncertainty](#Adding-uncertainty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = experiment['model'].predict(train_x)\n",
    "plot_dists((train_x[0], pred[1], 'blue') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatches\n",
    "\n",
    "One thing that could be causing trouble is to use the whole dataset (batch) in a single optimization step. In this experiment I will use a minibatch approach to ease the convergence and test the modelling capabilities.\n",
    "\n",
    "I start on a copy of the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiments['minibatches'] = {\n",
    "    'name': '02-minibatches',\n",
    "    'data': experiments['original']['data'].copy(),\n",
    "    'hyper': experiments['original']['hyper'].copy(),\n",
    "    'loss': experiments['original']['loss'],\n",
    "    'model': get_model01()  # Start with a fresh model\n",
    "}\n",
    "\n",
    "experiment['hyper']['optimizer'] = tf.keras.optimizers.Adam(  # Start with a fresh optimizer\n",
    "    learning_rate=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes\n",
    "\n",
    "I will use a 1024-sample batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "At first I trained the model for the same number of epochs and tracked the evolution of the **variance curve**, I saw it resembled the [original one](#Considerations) so I trained for a longer number of epochs until the model was properly fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment['hyper']['bs'] = 1024  # Reduce the batch size to ease the convergence\n",
    "experiment['hyper']['epochs'] *= 6  # There are no significant changes if 5 is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(\n",
    "    model=experiment['model'],\n",
    "    f_loss=experiment['loss'],\n",
    "    optimizer=experiment['hyper']['optimizer'],\n",
    "    batch_size=experiment['hyper']['bs'],\n",
    "    epochs=6000,#experiment['hyper']['epochs'],\n",
    "    x_train=train_x,\n",
    "    y_train=train_y,\n",
    "    f_callback=f_callback_basic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "tf.saved_model.save(experiment['model'], 'models/'+experiment['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Captured variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dists((\n",
    "    train_x[0],\n",
    "    experiment['model'].predict(train_x)[1],\n",
    "    'blue')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(experiment['model'], experiment['loss'], train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(experiment['model'], experiment['loss'], test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here comes Bayes\n",
    "\n",
    "#### Priors?\n",
    "\n",
    "#### The result\n",
    "\n",
    "This is Bayesian statistics, so...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Conseguir intervalos por acumulación\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html\n",
    "scipy.stats.norm.interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment['model']  = tf.saved_model.load('models/'+experiment['name'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "see if 2 distributions are the same for a x point\n",
    "from a synthetic perspective\n",
    "testing if 2 distributions are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Why does it takes so long to train?\n",
    "\n",
    "loss function, variance does not seem to affect much, other scale\n",
    "\n",
    "\n",
    "[variance](https://en.wikipedia.org/wiki/Variance) is a [second moment](https://en.wikipedia.org/wiki/Moment_(mathematics)) magnitude\n",
    "Proposition\n",
    "2-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I will save the data for the original experiment\n",
    "experiments['loss'] = {\n",
    "    'name': '02-loss',\n",
    "    'data': experiments['original']['data'].copy(),\n",
    "    'hyper': experiments['original']['hyper'].copy(),  # Do not keep the optimizer!\n",
    "    'model': get_model01()\n",
    "}\n",
    "\n",
    "experiment = experiments['loss']\n",
    "\n",
    "experiment['hyper']['optimizer'] = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss01(y_true, y_pred):  # Candidate 1\n",
    "    loss_y_pred = tf.reduce_mean(y_true[0]-y_pred[0])**2\n",
    "    loss_y_var = tf.reduce_mean( ((y_true[0]-y_pred[0])**2 - y_pred[1])**2 )\n",
    "    \n",
    "    \n",
    "    y_pred_sqr_error = (y_true[0]-y_pred[0])**2\n",
    "    return tf.reduce_mean(\n",
    "        y_pred_sqr_error +\n",
    "        (y_pred_sqr_error - y_pred[1])**2 \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO\n",
    "\n",
    "[] crear librería\n",
    "[]\n",
    "\n",
    "opciones\n",
    "2 steps\n",
    "función de pérdida\n",
    "\n",
    "\n",
    "pasar función de train a lib\n",
    "\n",
    "\n",
    "Creo que los errores de  varianza y predicción no están en la misma escala.\n",
    "¿Puedo balancearlos?\n",
    "\n",
    "\n",
    "Problemas\n",
    "\n",
    "Cómo ajustar una varianza de una predicción mala, no tiene sentido\n",
    "Magnitud predicción vs varianza \n",
    "Conclusión, al vuelo no tiene sentido.\n",
    "\n",
    "\n",
    "\n",
    "EXP2\n",
    "Modelo en dos fases\n",
    "Entrenar modelo predicción\n",
    "Entrenar modelo varianza\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO:\n",
    "Plot\n",
    "Gaussian tests against random points\n",
    "\n",
    "Relación con Bayesiano\n",
    " - ¿Dónde están los priors?\n",
    " - Parámetros de función\n",
    " - Prueba de aceptación"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
