## Mixture Models
En est√° p√°gina se explica **los modelos de mixturas** como soluci√≥n t√©cnica para aproximar distribuciones heterog√©neas de la variable respuesta. Este es el caso en el que sabemos que los datos u observaciones provienen de fuentes o procesos diferentes conocidos.


### Indice de contenidos
- [Introducci√≥n a la t√©cnica](#introduccion)
  - [MLE - Maximum Likelihood Estimation](#MLE)
- [Mixture Density Networks](#mdn)

<a name="introduccion"></a>
## Introducci√≥n

Un **modelo de mixturas** es un modelo probabil√≠stico que nos permite representar la presencia de sub-poblaciones de la poblaci√≥n general. Esta representaci√≥n de sub-poblaciones nos va a permitir construir un estimador m√°s robusto en el caso en el que la distribuci√≥n de la variable respuesta sea heterog√©nea


<a name="MLE"></a>
### MLE - Maximum Likelihood Estimation

El algoritmo de MLE o m√°xima verosimilitud nos permite obtener los par√°metros del modelo o distribuci√≥n que maximizan la probabibilidad de obtener unos datos dados.


<a name="mdn"></a>
### Mixture Density Networks

Las **redes de densidad mixta** (Bishop, 1994) es un tipo de red que combina las redes convencionales con la concepto de modelo de mixturas.

Entre las **aplicaciones m√°s destacadas** se encuentra la de Apple‚Äôs Siri en iOS 11 para reconocimiento de voz[2]. En [3] se puede ver su aplicaci√≥n en generaci√≥n de manuscritos y Amazon Forecast lo tiene dentro su suite de algoritmos incluidos en su plataforma.


<p align="center"><img src="/docs/assets/mdn/MDN.png" height="50" alt=‚ÄúMixture Density Network‚Äù /></p>
<p align="center">Mixture Density Network</p>

Formalmente la probabilidad condicionada de una red de mixturas tiene la siguiente forma:

<p align="center"><img src="/docs/assets/mdn/mdn_formula.png" height="50" alt=‚ÄúMixture Density Network‚Äù /></p>
<p align="center">Mixture Density Network</p>

En esta f√≥rmula los par√°metros tiene la siguiente sem√°ntica:

* **c se corresponde con el √≠ndice de la correspondiente mixtura**. Hay hasta C componentes de mixtura (e.g. distribuciones) por salida, siendo un parametro seleccionable.
* **‚ç∫ es el coeficiente de mezcla**. Para entender este coeficiente podemos imaginarnos los controles deslizantes que controlan la mezcla de C salidas diferentes de audio. Este par√°metro esta condicionado por la entrada x.
* **ùíü esta es la correspondiente distribuci√≥n de entrada a ser mezclada**. La distribuci√≥n puede ser elegida atendiendo al tipo de aplicaci√≥n.
* **Œª son los par√°metros de la distribuci√≥n ùíü**. En el caso denotamos ùíü como una distribuci√≥n gausiana, estos parametros corresponderian a Œª1 ser√≠a la media condicional mean Œº(x) y 
Œª2 la desviaci√≥n est√°ndar œÉ(x). Las distribuciones pueden tener distinto n√∫mero de par√°metros (e.g.: Bernoulli and Chi2 tienen 1 par√°metro, Beta tiene 2, y la gaussiana truncada tiene hasta 4 par√°metros) Estos son par√°metros que forman tambi√©n la salida de la red.



#### Referencias

[1] https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca

[2] Siri Team, Deep Learning for Siri‚Äôs Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis (2017)

[3] Alex Graves, Generating Sequences With Recurrent Neural Networks (2014)
